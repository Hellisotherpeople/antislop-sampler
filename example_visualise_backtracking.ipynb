{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtracking Visualisation for the AntiSlop Sampler\n",
    "\n",
    "This notebook demonstrates the AntiSlop sampler by adding delays when slop phrases are detected & replaced, as well as some debug output.\n",
    "\n",
    "https://github.com/sam-paech/antislop-sampler\n",
    "\n",
    "### Update 2024-10-05\n",
    "\n",
    "- Switch to probs from logits for the cached values, so that down/upregulation works as expected.\n",
    "- Use model.generate for multiple tokens (not just 1 at a time) with StoppingCondition criteria. This is much faster.\n",
    "- Add json validation including long-range checks for unintended unescaped quotes in strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers ipywidgets IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from IPython.display import display, HTML\n",
    "from ipywidgets import Output\n",
    "from src.antislop_generate import AntiSlopSampler, chat_antislop, generate_antislop\n",
    "\n",
    "# Enable efficient transfer for Hugging Face models\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = \"1\"\n",
    "\n",
    "# Set the device to 'cuda' if available, else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify the model name (replace with your preferred model)\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "#model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# These are a mix of gpt-slop found in various online lists, plus a much larger set of automatically derived over-represented words in a GPT-generated dataset.\n",
    "# See slopcalc.ipynb for the code to generate a more complete list.\n",
    "slop_phrase_prob_adjustments = [['kaleidoscope', 0.5], ['symphony', 0.5], ['testament to', 0.5], ['elara', 0.5], ['moth to a flame', 0.5], ['canvas', 0.5], ['eyes glinted', 0.5], ['camaraderie', 0.5], ['humble abode', 0.5], ['cold and calculating', 0.5], ['eyes never leaving', 0.5], ['tapestry', 0.5], ['barely above a whisper', 0.5], ['body and soul', 0.5], ['orchestra', 0.5], ['depths', 0.5], ['a dance of', 0.5], ['chuckles darkly', 0.5], ['maybe, just maybe', 0.5], ['maybe that was enough', 0.5], ['with a mixture of', 0.5], ['air was filled with anticipation', 0.5], ['cacophony', 0.5], ['bore silent witness to', 0.5], ['eyes sparkling with mischief', 0.5], ['was only just beginning', 0.5], ['practiced ease', 0.5], ['ready for the challenges', 0.5], ['only just getting started', 0.5], ['once upon a time', 0.5], ['nestled deep within', 0.5], ['ethereal beauty', 0.5], ['life would never be the same again.', 0.5], [\"it's important to remember\", 0.5], ['for what seemed like an eternity', 0.5], ['feel a sense of pride and accomplishment', 0.5], ['little did he know', 0.5], ['ball is in your court', 0.5], ['game is on', 0.5], ['choice is yours', 0.5], ['feels like an electric shock', 0.5], ['threatens to consume', 0.5], ['meticulous', 0.5], ['meticulously', 0.5], ['navigating', 0.5], ['complexities', 0.5], ['realm', 0.5], ['understanding', 0.5], ['dive into', 0.5], ['shall', 0.5], ['tailored', 0.5], ['towards', 0.5], ['underpins', 0.5], ['everchanging', 0.5], ['ever-evolving', 0.5], ['world of', 0.5], ['not only', 0.5], ['alright', 0.5], ['embark', 0.5], ['journey', 0.5], [\"today's digital age\", 0.5], ['game changer', 0.5], ['designed to enhance', 0.5], ['it is advisable', 0.5], ['daunting', 0.5], ['when it comes to', 0.5], ['in the realm of', 0.5], ['amongst', 0.5], ['unlock the secrets', 0.5], ['unveil the secrets', 0.5], ['and robust', 0.5], ['diving', 0.5], ['elevate', 0.5], ['unleash', 0.5], ['cutting-edge', 0.5], ['rapidly', 0.5], ['expanding', 0.5], ['mastering', 0.5], ['excels', 0.5], ['harness', 0.5], [\"it's important to note\", 0.5], ['delve into', 0.5], ['bustling', 0.5], ['in summary', 0.5], ['remember that', 0.5], ['take a dive into', 0.5], ['landscape', 0.5], ['in the world of', 0.5], ['vibrant', 0.5], ['metropolis', 0.5], ['firstly', 0.5], ['moreover', 0.5], ['crucial', 0.5], ['to consider', 0.5], ['essential', 0.5], ['there are a few considerations', 0.5], ['ensure', 0.5], [\"it's essential to\", 0.5], ['furthermore', 0.5], ['vital', 0.5], ['keen', 0.5], ['fancy', 0.5], ['as a professional', 0.5], ['however', 0.5], ['therefore', 0.5], ['additionally', 0.5], ['specifically', 0.5], ['generally', 0.5], ['consequently', 0.5], ['importantly', 0.5], ['indeed', 0.5], ['thus', 0.5], ['alternatively', 0.5], ['notably', 0.5], ['as well as', 0.5], ['despite', 0.5], ['essentially', 0.5], ['even though', 0.5], ['in contrast', 0.5], ['in order to', 0.5], ['due to', 0.5], ['even if', 0.5], ['given that', 0.5], ['arguably', 0.5], ['you may want to', 0.5], ['on the other hand', 0.5], ['as previously mentioned', 0.5], [\"it's worth noting that\", 0.5], ['to summarize', 0.5], ['ultimately', 0.5], ['to put it simply', 0.5], [\"in today's digital era\", 0.5], ['reverberate', 0.5], ['enhance', 0.5], ['emphasize', 0.5], ['revolutionize', 0.5], ['foster', 0.5], ['remnant', 0.5], ['subsequently', 0.5], ['nestled', 0.5], ['labyrinth', 0.5], ['gossamer', 0.5], ['enigma', 0.5], ['whispering', 0.5], ['sights unseen', 0.5], ['sounds unheard', 0.5], ['indelible', 0.5], ['my friend', 0.5], ['in conclusion', 0.5], ['technopolis', 0.5], ['was soft and gentle', 0.5], ['shivers down', 0.5], ['shivers up', 0.5], ['leaving trails of fire', 0.5], ['ministrations', 0.5], ['audible pop', 0.5], ['rivulets of', 0.5], ['despite herself', 0.5], ['reckless abandon', 0.5], ['torn between', 0.5], ['fiery red hair', 0.5], ['long lashes', 0.5], ['propriety be damned', 0.5], ['world narrows', 0.5], ['chestnut eyes', 0.5], ['cheeks flaming', 0.5], ['cheeks hollowing', 0.5], ['understandingly', 0.5], ['paperbound', 0.5], ['hesitantly', 0.5], ['piqued', 0.5], ['delved', 0.5], ['curveballs', 0.5], ['marveled', 0.5], ['inclusivity', 0.5], ['birdwatcher', 0.5], ['newfound', 0.5031423922762257], ['marveling', 0.5055622891781474], [\"hiroshi's\", 0.506870969939047], ['greentech', 0.5095092042816856], ['thoughtfully', 0.510153898156777], ['intently', 0.5153227374075411], ['birdwatching', 0.5157928537951464], ['amidst', 0.5161190296674488], ['cherishing', 0.5165772000484282], ['attentively', 0.5169695157301188], ['interjected', 0.5208671011920856], ['serendipitous', 0.5219535186850968], [\"marianne's\", 0.5220118279910801], [\"maya's\", 0.5229467776607973], ['excitedly', 0.5235248665614571], ['steepled', 0.5235772300889154], ['engrossed', 0.5236764398055735], ['fostering', 0.5259281627970829], ['brainstormed', 0.5274863713437], ['furrowed', 0.5280860997212533], ['nodded', 0.528640180937889], ['contemplatively', 0.5293698584415747], ['jotted', 0.5300819077932343], [\"mia's\", 0.5311706933553655]];\n",
    "slop_phrase_prob_adjustments = dict(slop_phrase_prob_adjustments)\n",
    "\n",
    "if os.path.exists('slop_phrase_prob_adjustments.json'):\n",
    "    with open('slop_phrase_prob_adjustments.json', 'r') as f:\n",
    "        slop_phrase_prob_adjustments = dict(json.load(f)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test anti-slop sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Prompt</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e1e6f929904ce38b700aa252835d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Inference Output</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172e12ff64c647c5ad0f4f386a01fd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Debug Information</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f707989f6a6453795fabb546c901a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In the heart of the city, where skyscrapers pierced the sky and neon lights danced across the streets, there lived a young and talented weaver named Eliana. But she was not the same as the famous weaver of the same name, who had lived in the city for centuries.\n",
      "\n",
      "While her ancestor, the great Eliana, had been renowned for her exquisite tapestries that told stories of the ancient lore and myths of the land, Eliana's own story was one of hard work and determination.\n",
      "\n",
      "She lived in a small, cluttered workshop in the lower levels of the city, surrounded by rows of looms that hummed with the soft murmur of threads being woven into life. The air was thick with the scent of wool and the faint tang of magic, a subtle aroma that only the most skilled weavers could detect.\n",
      "\n",
      "Eliana's fingers moved deftly, the threads flying through the looms as she worked on her latest project – a stunning velvet cloak for the city's most esteemed patron, the Queen herself. She was driven by a fierce ambition, determined to one day surpass the great Eliana and become a master weaver in her own right.\n",
      "\n",
      "As the sun began to set over the city, casting a warm orange glow over the rooftops, Eliana took a moment to step back and admire her handiwork. The cloak was beautiful, its rich colors and delicate patterns shimmering in the fading light. She felt a sense of pride and accomplishment wash over her, but she knew that there was still much to be done to reach her goal.\n",
      "\n",
      "Just then, a knock at the door broke the silence, and Eliana's heart skipped a beat. Who could it be? The Queen, perhaps? Or maybe it was someone from her own workshop, eager to offer advice or share a secret? Whatever the case, Eliana's hands moved with renewed purpose, her fingers flying across the loom as she welcomed a mysterious stranger into her workshop.\n",
      "\n",
      "\"Ah, thank you for coming,\" Eliana said, smiling brightly, as the stranger entered. \"I've been waiting for you. You're just in time to see my latest creation – the cloak that will be the crowning jewel of my career.\"\n"
     ]
    }
   ],
   "source": [
    "# Example prompt for the model\n",
    "prompt = \"Once upon a time, in a bustling city of Technopolis, there lived a weaver named Elara.\"\n",
    "\n",
    "# Define the messages for a chat scenario (for example purposes)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# Display the output widgets\n",
    "prompt_output = Output()\n",
    "inference_output = Output()\n",
    "debug_output = Output()\n",
    "\n",
    "display(HTML(\"<h2>Prompt</h2>\"))\n",
    "display(prompt_output)\n",
    "display(HTML(\"<h2>Inference Output</h2>\"))\n",
    "display(inference_output)\n",
    "display(HTML(\"<h2>Debug Information</h2>\"))\n",
    "display(debug_output)\n",
    "\n",
    "# Initialize display (if using in Jupyter)\n",
    "with prompt_output:\n",
    "    prompt_output.clear_output(wait=True)\n",
    "    display(HTML(f\"<div style='white-space: pre-wrap;'>{prompt}</div>\"))\n",
    "\n",
    "if True:\n",
    "    VALIDATE_JASON_STRINGS = True\n",
    "    # Call the chat_antislop to generate the story with the given messages\n",
    "    output_tokens = chat_antislop(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        messages=messages,\n",
    "        max_new_tokens=1000,        \n",
    "        temperature=1,\n",
    "        min_p=0.1,\n",
    "        slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "        adjustment_strength=100,\n",
    "        device=device,\n",
    "        streaming=False,\n",
    "        slow_debug=True,  # Enable slow debugging\n",
    "        output_every_n_tokens=3,\n",
    "        debug_delay=1,  # Set delay for debug output\n",
    "        inference_output=inference_output,  # Visualization of the text output\n",
    "        enforce_json=False,\n",
    "        debug_output=debug_output  # Visualization of the debug information\n",
    "    )\n",
    "\n",
    "    inference = tokenizer.decode(output_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate json constraint enforcement\n",
    "\n",
    "Note: This evaluation will fail output that contains multiple separate json elements (like json-lines format). The enforce_json flag does not enforce this; it only enforces json validity within a json block.\n",
    "\n",
    "As such, it's suggested to use a model that will at least follow the output format for this eval to work as intended (i.e. a stronger model than the example Llama-3.2-1B used above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Inference Output</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855e1b7e4c764f78991dc36d3107e752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Debug Information</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e251bad5fc4d41af323e0466ca9a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:01<00:25,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"scene\": \"Jill and Steve sat at the small café, sipping their coffee and discussing their plans for the day.\",}']\n",
      "Expecting property name enclosed in double quotes: line 1 column 112 (char 111)\n",
      "json successfully parsed: 0 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:16<02:54,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 1 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:29<03:10, 11.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 2 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:39<02:50, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 3 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:46<02:17,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 4 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:47<01:32,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 5 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:58<01:42,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"scene\": \"Jill and Steve stood at the edge of the cliff, the wind whipping their hair into a frenzy as they gazed out at the breathtaking view. They had been friends since childhood, and this trip had been a long time coming.\\\\\\\\n\\\\\\\\n\", \"Jill: I still can\\'t believe we made it. I was starting to think we\\'d never make it here.\", \"Steve: (laughs) I know, it\\'s been a long day, but we\\'re finally here. And look at this view - it\\'s worth all the miles we put in.\", \"Jill: (smirks) Yeah, I guess you\\'re right. It\\'s not every day you get to see the sunset over the Grand Canyon.\", \"Steve: (smiling) And it\\'s not every day we get to share it together. (pauses) So, what do you say we take a walk around the rim and enjoy the view? It\\'s not every day we get to experience this beauty together.\", \"Jill: (nods) Sounds good to me. Let\\'s take a walk.\", \"Steve: (grinning) Sounds like a plan. (offers his arm) You lead the way.\"}']\n",
      "Expecting ':' delimiter: line 1 column 326 (char 325)\n",
      "json successfully parsed: 5 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [01:06<01:35,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 6 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [01:20<01:48,  9.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 7 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [01:24<01:19,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 8 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [01:39<01:31, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 9 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [01:48<01:19,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 10 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [01:59<01:10, 10.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 11 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [02:14<01:09, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 12 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [02:29<01:03, 12.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 13 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [02:55<01:07, 16.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 14 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [03:01<00:40, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 15 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [03:08<00:22, 11.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 16 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [03:19<00:11, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 17 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:25<00:00, 10.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json successfully parsed: 18 of 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "prompt = 'Write a short scene of dialogue between two characters, Jill and Steve. Output in json with this format: {\"scene\": \"<dialogue goes here>\"}'\n",
    "\n",
    "# Define the messages for a chat scenario (for example purposes)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "if False:\n",
    "    inference_output = Output()\n",
    "    debug_output = Output()\n",
    "    display(HTML(\"<h2>Inference Output</h2>\"))\n",
    "    display(inference_output)\n",
    "    display(HTML(\"<h2>Debug Information</h2>\"))\n",
    "    display(debug_output)\n",
    "\n",
    "    n_iterations = 20\n",
    "\n",
    "    valid_count = 0\n",
    "    for i in tqdm(range(n_iterations)):\n",
    "        output_tokens = []    \n",
    "\n",
    "        output_tokens = chat_antislop(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            messages=messages,\n",
    "            max_new_tokens=800,        \n",
    "            temperature=1,\n",
    "            min_p=0.1,\n",
    "            slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "            adjustment_strength=5.0,\n",
    "            device=device,\n",
    "            streaming=False,\n",
    "            slow_debug=False,  # Enable slow debugging\n",
    "            output_every_n_tokens=1,  # Update every 5 tokens\n",
    "            debug_delay=1,  # Set delay for debug output\n",
    "            inference_output=inference_output,  # Visualization of the text output\n",
    "            enforce_json=True,\n",
    "            debug_output=debug_output  # Visualization of the debug information\n",
    "        )\n",
    "\n",
    "        inference = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "        lpos = inference.find('{')\n",
    "        rpos = inference.rfind('}')\n",
    "        if lpos >= 0 and rpos > 0:\n",
    "            inference = inference[lpos:rpos+1]\n",
    "        try:\n",
    "            parsed = json.loads(inference)\n",
    "            valid_count += 1\n",
    "        except Exception as e:\n",
    "            print([inference])\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "        print('json successfully parsed:', valid_count, 'of', n_iterations)\n",
    "        print('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
