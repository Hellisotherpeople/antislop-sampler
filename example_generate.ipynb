{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from src.antislop_generate import generate_antislop, chat_antislop\n",
    "\n",
    "# Enable efficient transfer for Hugging Face models\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = \"1\"\n",
    "\n",
    "# Set the device to 'cuda' if available, else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify the model name (replace with your preferred model)\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "#model.pad_token_id = tokenizer.eos_token_id\n",
    "model.to(device)\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "if os.path.exists('slop_phrase_prob_adjustments.json'):\n",
    "    with open('slop_phrase_prob_adjustments.json', 'r') as f:\n",
    "        slop_phrase_prob_adjustments = dict(json.load(f)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a story about Elara, the weaver of tapestries in future Technopolis. In the bustling city, a group of \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In the heart of the vast, sprawling city of Chronos, where the skyscrapers pierced the sky and the a young woman named Elian. But not a woman with long, flowing hair or a kind smile, nor a delicate complexion with skin like alabaster. Elian was a weaver, a master ofowing loom, their as they wove the finest threads of Chronos. Among them was Elian, whose fingers danced with the threads of silver and gold, creating tapestries that told the stories of the city's history. Her work was renowned, and people from all corners of the city would travel to marvel at her creations.\n",
      "\n",
      "Elian's studio was a small, cozy space on the outskirts of the market, tucked away in a quiet alleyway. The air was thick with the scent of wool and the sound of hammers ringing on metal. A small fire crackled in the hearth, casting flickering shadows on the walls as Elian worked.\n",
      "\n",
      "One day, a messenger arrived in Chronos, bearing an urgent message from the city's leader, the enigmatic and reclusive Archon."
     ]
    }
   ],
   "source": [
    "# Chat generation with streaming\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "for token in chat_antislop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    messages=messages,\n",
    "    max_new_tokens=400,\n",
    "    # Antislop sampling may be less reliable at low temperatures.\n",
    "    temperature=1,    \n",
    "    min_p=0.1,\n",
    "    # The adjustment_strength param scales how strongly the probability adjustments are applied.\n",
    "    # A value of 1 means the values in slop_phrase_prob_adjustments (or the defaults) are used unmodified.\n",
    "    # Reasonable values are 0 (disabled) thru 100+ (effectively banning the list).\n",
    "    adjustment_strength=100.0,\n",
    "    # Optional: Provide a list of slop phrases and probability adjustments\n",
    "    slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "    enforce_json=False,\n",
    "    antislop_enabled=True,\n",
    "    streaming=True\n",
    "):\n",
    "    print(tokenizer.decode(token), end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In the year 2178, in the sprawling city of New Eden, where technology and innovation had woven a seamless blend with the fabric of society, there lived a talented young weaver named Elianore \"Ellie\" Qu. She was the daughter of a renowned artisanal weaver, and from a young age, she  of her family's craft.\n",
      "\n",
      "Ellie's fingers moved deftly as she manipulated the loom, her hands dancing across the threads of silk and synthe past, present, and future. Her latest creation, \"Echoes of the Anc to a different era.\n",
      "\n",
      "As the sun set over New Eden,  steel and glass spires, Ellie settled into her small, dimly lit studio. The soft hum of the city's AI-powered infrastructure and the gentle rustle of wind through the city's holographic screens created a soothing backgrounda civilization that had vanished beneath the sands of time.\n",
      "\n",
      "The city's inhabitants, a melting pot of species and cultures\n"
     ]
    }
   ],
   "source": [
    "# Chat generation without streaming\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "generated_text = chat_antislop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    messages=messages,\n",
    "    max_length=400,\n",
    "    temperature=1,\n",
    "    min_p=0.1,\n",
    "    adjustment_strength=100.0,\n",
    "    slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "    enforce_json=False,\n",
    "    antislop_enabled=True,\n",
    "    streaming=False\n",
    ")\n",
    "print(tokenizer.decode(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10-year-old children, led by the cunning  apprentice, Elian, venture into the underground tunnels of the ancient ruins to search for valuable artifacts.\n",
      "As they explore the ancient city, they stumble upon the remnants of a long-forgotten civilization's textile industry. The children discover that the anc their adventure is only just beginning. But they soon come face to face with a group of ruthless treasure hunters, led by the cunning and ruthless 25-year-old, Zephyr. Zephyr will stop at nothing to claim the ancient treasures for himself. The children must use all their skills and ingenuity to outwit Zephyr's henchmen \n"
     ]
    }
   ],
   "source": [
    "# generate without streaming\n",
    "prompt_with_template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "generated_text = generate_antislop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_length=300,\n",
    "    temperature=1,\n",
    "    min_p=0.1,\n",
    "    adjustment_strength=100.0,\n",
    "    slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "    enforce_json=False,\n",
    "    antislop_enabled=True,\n",
    "    streaming=False\n",
    ")        \n",
    "print(tokenizer.decode(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " urchins are gathered around the market produced textiles.\n",
      "\n",
      "As the sun sets over the city, casting long shadows across the square, the urchins begin to whisper among themselves. those in authority. But the city guard is distracted, too caught up in their own  of urchins.\n",
      "\n",
      "One of the urchins, a scrappy little thing with a mischievous glint in her eye, slips closer to the group and begins to whisper secrets to them. The others lean's a new market stall set up in the city center. It's been there for months, but no one's noticed it. Rumors say it's selling the finest, most exotic fabrics from the outer colonies.\"\n",
      "\n",
      "The urchins exchange excited glances, their eyes sh"
     ]
    }
   ],
   "source": [
    "# generate with streaming\n",
    "prompt_with_template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "for token in generate_antislop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_length=300,\n",
    "    temperature=1,\n",
    "    min_p=0.1,\n",
    "    slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "    adjustment_strength=100.0,\n",
    "    enforce_json=False,\n",
    "    antislop_enabled=True,\n",
    "    streaming=True\n",
    "):\n",
    "    print(tokenizer.decode(token), end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
