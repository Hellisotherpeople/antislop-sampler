{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies:\n",
    "\n",
    "#!pip install transformers ipywidgets IPython\n",
    "\n",
    "# If running on Colab:\n",
    "#!git clone https://github.com/sam-paech/antislop-sampler.git\n",
    "#!mv antislop-sampler/src ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from src.antislop_generate import generate_antislop, chat_antislop\n",
    "\n",
    "# Enable efficient transfer for Hugging Face models\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = \"1\"\n",
    "\n",
    "# Set the device to 'cuda' if available, else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify the model name (replace with your preferred model)\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "#model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "#model.pad_token_id = tokenizer.eos_token_id\n",
    "model.to(device)\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "if os.path.exists('slop_phrase_prob_adjustments.json'):\n",
    "    with open('slop_phrase_prob_adjustments.json', 'r') as f:\n",
    "        slop_phrase_prob_adjustments = dict(json.load(f)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a story about Elara, the weaver of tapestries in future Technopolis. In the bustling city, a group of \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "In the year 2178, in the futuristic sprawling city of New Elysium, a group of skilled artisans, known as the Guild of Weavers, toiled away in the city's central district. Among them was a talented young woman named Elianore \"Lara\" Elwes, a weaver of extraordinary talent and dedication.\n",
      "\n",
      "Lara's fingers moved deftly, her eyes weaving a complex pattern of colors and textures into every strand of thread. Her home, a small studio apartment within the city's walls, was a sanctuary, filled with the scent of wool and the soft glow of luminescent fibers. As the Guild of Weavers, Lara was renowned for her breathtaking tapestries, which adorned the grand halls of New Elysium's most prominent residences.\n",
      "\n",
      "One day, a mysterious message arrived at the Guild's headquarters, a sleek, aerodynamic tower that pierced the city's sky. The message, encrypted and unsigned, was from an unknown sender, requesting a meeting with the Guild's leader, the enigmatic and reclusive master weaver, Arachne.\n",
      "\n",
      "Rumors whispered among the weavers that Arachne was searching for someone with a rare gift â€“ the ability to see the hidden patterns that underlay the city's fabric. It was said that those who possessed this gift could unravel the threads of fate, revealing the secrets of New Elysium's future.\n",
      "\n",
      "Lara, feeling an inexplicable sense of excitement and trepidation, agreed to meet with Arachne. As she entered the tower, she was greeted by a figure shrouded in shadows. The air was thick with the scent of incense, and the soft glow of luminescent fibers illuminated the room.\n",
      "\n",
      "Arachne, a woman with eyes that burned like stars, revealed herself to Lara. \"I have been watching you, Elwes,\" she said, her voice low and soothing. \"I believe you possess the gift we"
     ]
    }
   ],
   "source": [
    "# Chat generation with streaming\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "for token in chat_antislop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    messages=messages,\n",
    "    max_new_tokens=400,\n",
    "    # Antislop sampling may be less reliable at low temperatures.\n",
    "    temperature=1,    \n",
    "    min_p=0.1,\n",
    "    # The adjustment_strength param scales how strongly the probability adjustments are applied.\n",
    "    # A value of 1 means the values in slop_phrase_prob_adjustments (or the defaults) are used unmodified.\n",
    "    # Reasonable values are 0 (disabled) thru 100+ (effectively banning the list).\n",
    "    adjustment_strength=100.0,\n",
    "    # Optional: Provide a list of slop phrases and probability adjustments\n",
    "    slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "    enforce_json=False,\n",
    "    antislop_enabled=True,\n",
    "    streaming=True\n",
    "):\n",
    "    print(tokenizer.decode(token), end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "In the heart of of future **Tenopolis**, where skyscrapers pierced the sky and neon lights danced across the rooftops, a lone figure emerged from the crowd. Her name was **Aria**, but she was known to the people as **La Weaver**, the mysterious weaver of tapestries.\n",
      "\n",
      "Aria lived in a small, cozy shop in the heart of the city, surrounded by the hum of machinery and the chatter of pedestrians. The sign above her door read \"La Weaver's Masterpieces,\" and the windows were always filled with an array of colorful tapestries, each one telling a different story of the city's history.\n",
      "\n",
      "La Weaver's talent lay in her ability to weave more than just fabric. She had a deep connection with the **Energy Threads**, the lifeblood of the city, which flowed through every building, streetlamp, and alleyway. By manipulating these threads, she could create breathtaking tapestries that told the stories of Tenopolis's past, present, and future.\n",
      "\n",
      "One day, a group of five individuals arrived at La Weaver's shop, seeking her expertise. They were a diverse bunch, each with their own unique skills and motivations. There was **Kael**, a skilled **Mechanist** who could repair any machine; **Lena**, a **Cultural Ambassador** who was fluent in five languages and could decipher ancient texts; **Arin**, a **Cybernetic Engineer** who could hack into any system; **Niamh**, a **Mystic** who possessed ancient knowledge of the city's secrets; and **Thorne**, a **Streetwise Survivor** who had lived through the city\n"
     ]
    }
   ],
   "source": [
    "# Chat generation without streaming\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "generated_text = chat_antislop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    messages=messages,\n",
    "    max_length=400,\n",
    "    temperature=1,\n",
    "    min_p=0.1,\n",
    "    adjustment_strength=100.0,\n",
    "    slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "    enforce_json=False,\n",
    "    antislop_enabled=True,\n",
    "    streaming=False\n",
    ")\n",
    "print(tokenizer.decode(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " urchins had made a game out of stealing the latest fabrics for their own art projects. They would scamper through the crowded marketplaces and snatch scraps from the weavers' workshops.\n",
      "\n",
      "One day, a young apprentice named Eira, who worked for the reclusive and enigmatic weaver, Lyra, was tasked with retrieving a particularly valuable fabric. Lyra had given the fabric to a mysterious client, and Eira was tasked with delivering it to the client, who was rumored to be a prominent member of the upper class.\n",
      "\n",
      "Eira had heard stories about the upper class and their love of fine fabrics. She was determined to prove herself and earn her place among the upper class. As she made her way through the crowded marketplaces, she kept a sharp eye out for the urchins. They were always quick to snatch a quick item, but Eira was determined to outsmart them.\n",
      "\n",
      "As she approached the market stall, she saw the urchins gathered around a colorful display of fabrics. Eira's heart sank, but she steeled herself and began to scan the display for the valuable fabric. She spotted it, a bright blue silk that was woven with threads of silver and gold.\n",
      "\n",
      "But before she could grab the fabric, one of the urchins snatched it from her hand, and Eira was forced to\n"
     ]
    }
   ],
   "source": [
    "# generate without streaming\n",
    "prompt_with_template = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "generated_text = generate_antislop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_length=300,\n",
    "    temperature=1,\n",
    "    min_p=0.1,\n",
    "    adjustment_strength=100.0,\n",
    "    slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "    enforce_json=False,\n",
    "    antislop_enabled=True,\n",
    "    streaming=False\n",
    ")        \n",
    "print(tokenizer.decode(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5 young apprentices of the Weaver Guild, all skilled in their own unique ways, arrive at the Guild's headquarters. As they enter the grand hall, they are greeted by the enigmatic and reclusive Guildmaster, Ezra.\n",
      "\n",
      "Ezra's eyes gleam with a knowing light as he surveys the young apprentices before him. \"Ah, the new blood has arrived. I have been expecting you. You three are the last of the old guard, I believe? The ones who refused to adapt to the new ways of our world.\"\n",
      "\n",
      "The apprentices exchange nervous glances, unsure of what to expect from the enigmatic Guildmaster. They are a diverse group, each with their own unique skillset:\n",
      "\n",
      "* Lyra, the youngest and most impulsive, is an expert in elemental magic, able to conjure powerful gusts of wind and whirlwinds with a mere thought.\n",
      "* Kael, the oldest and most stoic, is a master of mechanical engineering, able to craft complex devices and machines with ease.\n",
      "* Mira, the most reserved of the group, is an expert in the ancient art of divination, able to interpret the omens and portents of the universe.\n",
      "* Arin, the most charismatic of the group, is a skilled diplomat and negotiator, able to weave words into powerful spells of persuasion.\n",
      "* Z"
     ]
    }
   ],
   "source": [
    "# generate with streaming\n",
    "prompt_with_template = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "for token in generate_antislop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_length=300,\n",
    "    temperature=1,\n",
    "    min_p=0.1,\n",
    "    slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "    adjustment_strength=100.0,\n",
    "    enforce_json=False,\n",
    "    antislop_enabled=True,\n",
    "    streaming=True\n",
    "):\n",
    "    print(tokenizer.decode(token), end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
