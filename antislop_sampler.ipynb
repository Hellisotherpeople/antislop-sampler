{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtracking Visualisation for the AntiSlop Sampler\n",
    "\n",
    "Run the notebook & scroll all the way to the bottom to see the sampler in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import traceback\n",
    "from typing import List, Dict, Tuple, Generator, Set, Union\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from ipywidgets import Output\n",
    "import numpy as np\n",
    "\n",
    "# Enable efficient transfer for Hugging Face models\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = \"1\"\n",
    "\n",
    "# Set the device to 'cuda' if available, else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify the model name (replace with your preferred model)\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# These are a mix of gpt-slop found in various online lists, plus a much larger set of automatically derived over-represented words in a GPT-generated dataset.\n",
    "# See slopcalc.ipynb for the code to generate a more complete list.\n",
    "slop_phrase_prob_adjustments = [['kaleidoscope', 0.5], ['symphony', 0.5], ['testament to', 0.5], ['elara', 0.5], ['moth to a flame', 0.5], ['canvas', 0.5], ['eyes glinted', 0.5], ['camaraderie', 0.5], ['humble abode', 0.5], ['cold and calculating', 0.5], ['eyes never leaving', 0.5], ['tapestry', 0.5], ['barely above a whisper', 0.5], ['body and soul', 0.5], ['orchestra', 0.5], ['depths', 0.5], ['a dance of', 0.5], ['chuckles darkly', 0.5], ['maybe, just maybe', 0.5], ['maybe that was enough', 0.5], ['with a mixture of', 0.5], ['air was filled with anticipation', 0.5], ['cacophony', 0.5], ['bore silent witness to', 0.5], ['eyes sparkling with mischief', 0.5], ['was only just beginning', 0.5], ['practiced ease', 0.5], ['ready for the challenges', 0.5], ['only just getting started', 0.5], ['once upon a time', 0.5], ['nestled deep within', 0.5], ['ethereal beauty', 0.5], ['life would never be the same again.', 0.5], [\"it's important to remember\", 0.5], ['for what seemed like an eternity', 0.5], ['feel a sense of pride and accomplishment', 0.5], ['little did he know', 0.5], ['ball is in your court', 0.5], ['game is on', 0.5], ['choice is yours', 0.5], ['feels like an electric shock', 0.5], ['threatens to consume', 0.5], ['meticulous', 0.5], ['meticulously', 0.5], ['navigating', 0.5], ['complexities', 0.5], ['realm', 0.5], ['understanding', 0.5], ['dive into', 0.5], ['shall', 0.5], ['tailored', 0.5], ['towards', 0.5], ['underpins', 0.5], ['everchanging', 0.5], ['ever-evolving', 0.5], ['world of', 0.5], ['not only', 0.5], ['alright', 0.5], ['embark', 0.5], ['journey', 0.5], [\"today's digital age\", 0.5], ['game changer', 0.5], ['designed to enhance', 0.5], ['it is advisable', 0.5], ['daunting', 0.5], ['when it comes to', 0.5], ['in the realm of', 0.5], ['amongst', 0.5], ['unlock the secrets', 0.5], ['unveil the secrets', 0.5], ['and robust', 0.5], ['diving', 0.5], ['elevate', 0.5], ['unleash', 0.5], ['cutting-edge', 0.5], ['rapidly', 0.5], ['expanding', 0.5], ['mastering', 0.5], ['excels', 0.5], ['harness', 0.5], [\"it's important to note\", 0.5], ['delve into', 0.5], ['bustling', 0.5], ['in summary', 0.5], ['remember that', 0.5], ['take a dive into', 0.5], ['landscape', 0.5], ['in the world of', 0.5], ['vibrant', 0.5], ['metropolis', 0.5], ['firstly', 0.5], ['moreover', 0.5], ['crucial', 0.5], ['to consider', 0.5], ['essential', 0.5], ['there are a few considerations', 0.5], ['ensure', 0.5], [\"it's essential to\", 0.5], ['furthermore', 0.5], ['vital', 0.5], ['keen', 0.5], ['fancy', 0.5], ['as a professional', 0.5], ['however', 0.5], ['therefore', 0.5], ['additionally', 0.5], ['specifically', 0.5], ['generally', 0.5], ['consequently', 0.5], ['importantly', 0.5], ['indeed', 0.5], ['thus', 0.5], ['alternatively', 0.5], ['notably', 0.5], ['as well as', 0.5], ['despite', 0.5], ['essentially', 0.5], ['even though', 0.5], ['in contrast', 0.5], ['in order to', 0.5], ['due to', 0.5], ['even if', 0.5], ['given that', 0.5], ['arguably', 0.5], ['you may want to', 0.5], ['on the other hand', 0.5], ['as previously mentioned', 0.5], [\"it's worth noting that\", 0.5], ['to summarize', 0.5], ['ultimately', 0.5], ['to put it simply', 0.5], [\"in today's digital era\", 0.5], ['reverberate', 0.5], ['enhance', 0.5], ['emphasize', 0.5], ['revolutionize', 0.5], ['foster', 0.5], ['remnant', 0.5], ['subsequently', 0.5], ['nestled', 0.5], ['labyrinth', 0.5], ['gossamer', 0.5], ['enigma', 0.5], ['whispering', 0.5], ['sights unseen', 0.5], ['sounds unheard', 0.5], ['indelible', 0.5], ['my friend', 0.5], ['in conclusion', 0.5], ['technopolis', 0.5], ['was soft and gentle', 0.5], ['shivers down', 0.5], ['shivers up', 0.5], ['leaving trails of fire', 0.5], ['ministrations', 0.5], ['audible pop', 0.5], ['rivulets of', 0.5], ['despite herself', 0.5], ['reckless abandon', 0.5], ['torn between', 0.5], ['fiery red hair', 0.5], ['long lashes', 0.5], ['propriety be damned', 0.5], ['world narrows', 0.5], ['chestnut eyes', 0.5], ['cheeks flaming', 0.5], ['cheeks hollowing', 0.5], ['understandingly', 0.5], ['paperbound', 0.5], ['hesitantly', 0.5], ['piqued', 0.5], ['delved', 0.5], ['curveballs', 0.5], ['marveled', 0.5], ['inclusivity', 0.5], ['birdwatcher', 0.5], ['newfound', 0.5031423922762257], ['marveling', 0.5055622891781474], [\"hiroshi's\", 0.506870969939047], ['greentech', 0.5095092042816856], ['thoughtfully', 0.510153898156777], ['intently', 0.5153227374075411], ['birdwatching', 0.5157928537951464], ['amidst', 0.5161190296674488], ['cherishing', 0.5165772000484282], ['attentively', 0.5169695157301188], ['interjected', 0.5208671011920856], ['serendipitous', 0.5219535186850968], [\"marianne's\", 0.5220118279910801], [\"maya's\", 0.5229467776607973], ['excitedly', 0.5235248665614571], ['steepled', 0.5235772300889154], ['engrossed', 0.5236764398055735], ['fostering', 0.5259281627970829], ['brainstormed', 0.5274863713437], ['furrowed', 0.5280860997212533], ['nodded', 0.528640180937889], ['contemplatively', 0.5293698584415747], ['jotted', 0.5300819077932343], [\"mia's\", 0.5311706933553655]];\n",
    "slop_phrase_prob_adjustments = dict(slop_phrase_prob_adjustments)\n",
    "\n",
    "if os.path.exists('slop_phrase_prob_adjustments.json'):\n",
    "    with open('slop_phrase_prob_adjustments.json', 'r') as f:\n",
    "        slop_phrase_prob_adjustments = dict(json.load(f)[:500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precompute_starting_tokens(\n",
    "    tokenizer: PreTrainedTokenizer, slop_phrase_prob_adjustments: Dict[str, float]\n",
    ") -> Dict[Tuple[int, ...], Set[int]]:\n",
    "    starting_tokens_lookup = {}\n",
    "\n",
    "    for word in slop_phrase_prob_adjustments.keys():\n",
    "        variants = [\n",
    "            word.lower(),\n",
    "            word.capitalize(),\n",
    "            word.upper(),\n",
    "            f\" {word.lower()}\",\n",
    "            f\" {word.capitalize()}\",\n",
    "            f\" {word.upper()}\",\n",
    "        ]\n",
    "\n",
    "        for variant in variants:\n",
    "            token_ids = tokenizer.encode(variant, add_special_tokens=False)\n",
    "            starting_tokens = set()\n",
    "            if token_ids:\n",
    "                starting_tokens.add(token_ids[0])\n",
    "                first_token_decoded = tokenizer.decode(token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                for i in range(len(first_token_decoded) - 1):\n",
    "                    prefix = first_token_decoded[:-(i + 1)]\n",
    "                    encoded_prefix = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "                    if encoded_prefix:\n",
    "                        starting_tokens.add(encoded_prefix[0])\n",
    "\n",
    "                starting_tokens_lookup[tuple(token_ids)] = starting_tokens\n",
    "\n",
    "    return starting_tokens_lookup\n",
    "\n",
    "# Precompute starting tokens\n",
    "starting_tokens_lookup = precompute_starting_tokens(tokenizer, slop_phrase_prob_adjustments)\n",
    "\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from ipywidgets import Output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SlopPhraseStoppingCriteria:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, token_sequences: Dict[Tuple[int, ...], float], max_sequence_length: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_sequences = token_sequences\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "    def _detect_disallowed_sequence(self, generated_sequence: List[int]) -> Tuple[Tuple[int, ...], int]:\n",
    "        for seq_length in range(self.max_sequence_length, 0, -1):            \n",
    "            if len(generated_sequence) < seq_length:\n",
    "                continue\n",
    "            candidate_sequence = tuple(generated_sequence[-seq_length:])\n",
    "            if candidate_sequence in self.token_sequences:\n",
    "                start_pos = len(generated_sequence) - seq_length\n",
    "                return candidate_sequence, start_pos\n",
    "        return None, -1\n",
    "\n",
    "class AdvancedCustomWordSampler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        slop_phrase_prob_adjustments: Dict[str, float],\n",
    "        starting_tokens_lookup: Dict[Tuple[int, ...], Set[int]],\n",
    "        adjustment_strength: float = 1.0,\n",
    "        device: torch.device = torch.device('cuda'),\n",
    "        slow_debug: bool = False,\n",
    "        output_every_n_tokens: int = 1,\n",
    "        debug_delay: float = 2.0,\n",
    "        inference_output=None,\n",
    "        debug_output=None,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.slop_phrase_prob_adjustments = slop_phrase_prob_adjustments\n",
    "        self.starting_tokens_lookup = starting_tokens_lookup\n",
    "        self.adjustment_strength = adjustment_strength\n",
    "        self.device = device\n",
    "        self.slow_debug = slow_debug\n",
    "        self.output_every_n_tokens = output_every_n_tokens\n",
    "        self.debug_delay = debug_delay\n",
    "\n",
    "        self.token_sequences = self._prepare_token_sequences()\n",
    "        self.max_sequence_length = max(len(seq) for seq in self.token_sequences.keys())\n",
    "\n",
    "        self.stopping_criteria = SlopPhraseStoppingCriteria(tokenizer, self.token_sequences, self.max_sequence_length)\n",
    "\n",
    "        self.downregulated_positions = {}  # Key: position, Value: set of sequences\n",
    "\n",
    "        self.logit_cache = {}\n",
    "\n",
    "        # Output widgets\n",
    "        self.inference_output = inference_output\n",
    "        self.debug_output = debug_output\n",
    "\n",
    "    def _prepare_token_sequences(self) -> Dict[Tuple[int, ...], float]:\n",
    "        token_sequences = {}\n",
    "        for word, prob_adjustment_factor in self.slop_phrase_prob_adjustments.items():\n",
    "            variants = [\n",
    "                word.lower(),\n",
    "                word.capitalize(),\n",
    "                word.upper(),\n",
    "                f\" {word.lower()}\",\n",
    "                f\" {word.capitalize()}\",\n",
    "                f\" {word.upper()}\",\n",
    "            ]\n",
    "            for variant in variants:\n",
    "                token_ids = tuple(self.tokenizer.encode(variant, add_special_tokens=False))\n",
    "                if token_ids:\n",
    "                    token_sequences[token_ids] = prob_adjustment_factor\n",
    "        return token_sequences\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_length: int = None,\n",
    "        max_new_tokens: int = None,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int = None,\n",
    "        top_p: float = None,\n",
    "        min_p: float = None,\n",
    "    ) -> Generator[List[int], None, None]:\n",
    "        \"\"\"\n",
    "        Generates text in a streaming fashion with custom downregulation and backtracking.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The initial text prompt.\n",
    "            max_length (int, optional): The maximum length of the generated text.\n",
    "            max_new_tokens (int, optional): The maximum number of new tokens to generate.\n",
    "            temperature (float): Sampling temperature.\n",
    "            top_k (int): Top-k filtering.\n",
    "            top_p (float): Top-p (nucleus) filtering.\n",
    "            min_p (float): Minimum probability filtering.\n",
    "\n",
    "        Yields:\n",
    "            Generator[List[int], None, None]: Yields generated token sequences.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Encode the prompt\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        generated_sequence = input_ids[0].tolist()\n",
    "        prompt_length = len(generated_sequence)\n",
    "        current_position = len(generated_sequence)  # Tracks the current position in the sequence\n",
    "        output_tokens_counter = 0\n",
    "        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n",
    "\n",
    "        num_new_tokens = 0\n",
    "\n",
    "        \n",
    "\n",
    "        while True:\n",
    "            if max_length is not None and len(generated_sequence) >= max_length:\n",
    "                break\n",
    "            if max_new_tokens is not None and num_new_tokens >= max_new_tokens:\n",
    "                break\n",
    "\n",
    "            current_input_ids = torch.tensor([generated_sequence], device=self.device)\n",
    "\n",
    "            regenerating = False\n",
    "            if current_position in self.logit_cache:\n",
    "                # We backtracked and want to use the cached logits\n",
    "                next_token_logits = self.logit_cache[current_position]\n",
    "                regenerating = True\n",
    "            else:\n",
    "                outputs = self.model.generate(\n",
    "                    current_input_ids,\n",
    "                    attention_mask = torch.ones_like(current_input_ids),\n",
    "                    max_new_tokens=1,\n",
    "                    do_sample=False,\n",
    "                    temperature = 1, # we apply temp ourselves after this\n",
    "                    pad_token_id = pad_token_id,\n",
    "                    num_return_sequences=1,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                )\n",
    "                next_token_logits = outputs.scores[0]\n",
    "                self.logit_cache[current_position] = next_token_logits.clone()\n",
    "\n",
    "                # Apply temperature\n",
    "                # note: we don't want to re-apply temp if using cached logits\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "\n",
    "            # Apply min_p, top-k and top-p filtering\n",
    "            filtered_logits = self._filter_logits(next_token_logits, top_k, top_p, min_p)\n",
    "\n",
    "            # Sample the next token\n",
    "            probs = torch.softmax(filtered_logits, dim=-1)\n",
    "            next_token_index = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = next_token_index.item()\n",
    "\n",
    "            if regenerating and self.slow_debug:\n",
    "                alt_token = self.tokenizer.decode(next_token, skip_special_tokens=True)\n",
    "                debug_info = f\"Alternate token: {alt_token}\"\n",
    "                self._display_debug(debug_info)\n",
    "                if self.slow_debug:\n",
    "                    time.sleep(self.debug_delay)\n",
    "\n",
    "            # Append the new token to the sequence\n",
    "            generated_sequence.append(next_token)\n",
    "            current_position += 1\n",
    "            output_tokens_counter += 1\n",
    "            num_new_tokens += 1\n",
    "\n",
    "            # Clean up the logits cache\n",
    "            to_del = [key for key in self.logit_cache if key < current_position - self.max_sequence_length - 5]\n",
    "            for key in to_del:\n",
    "                del self.logit_cache[key]\n",
    "\n",
    "            # Yield the current text chunk            \n",
    "            if output_tokens_counter >= self.output_every_n_tokens:\n",
    "                output_tokens_counter = 0\n",
    "                current_text = self.tokenizer.decode(generated_sequence[prompt_length:])\n",
    "                if self.inference_output:\n",
    "                    with self.inference_output:\n",
    "                        self.inference_output.clear_output(wait=True)\n",
    "                        display(HTML(f\"<div style='white-space: pre-wrap;'>{current_text}</div>\"))\n",
    "                yield generated_sequence  # Yield the generated token sequence\n",
    "\n",
    "            # Check for end-of-sequence token\n",
    "            if next_token == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # After adding the token, check for disallowed sequences\n",
    "            matched_sequence, start_pos = self.stopping_criteria._detect_disallowed_sequence(generated_sequence)\n",
    "\n",
    "            if matched_sequence:\n",
    "                # Downregulate the relevant tokens at the start_pos\n",
    "                adjustment = self.token_sequences[matched_sequence]\n",
    "                word = self.tokenizer.decode(torch.tensor(matched_sequence))\n",
    "\n",
    "                # Display debug information\n",
    "                debug_info = f\"Replacing '{word}'\"\n",
    "                self._display_debug(debug_info)\n",
    "\n",
    "                if self.slow_debug:\n",
    "                    time.sleep(self.debug_delay)\n",
    "                    if self.debug_output:\n",
    "                        with self.debug_output:\n",
    "                            self.debug_output.clear_output(wait=True)\n",
    "\n",
    "                # Identify starting tokens to downregulate\n",
    "                starting_tokens = self.starting_tokens_lookup.get(matched_sequence, set())\n",
    "\n",
    "                for token_id in starting_tokens:\n",
    "                    self.logit_cache[start_pos][:, token_id] *= adjustment ** self.adjustment_strength\n",
    "\n",
    "                # Record that this sequence has been downregulated at this position\n",
    "                if start_pos not in self.downregulated_positions:\n",
    "                    self.downregulated_positions[start_pos] = set()\n",
    "                self.downregulated_positions[start_pos].add(matched_sequence)\n",
    "\n",
    "                # Check if the starting token would still be selected after downregulation\n",
    "                slop_phrase_starting_token = generated_sequence[start_pos]\n",
    "                if torch.argmax(self.logit_cache[start_pos]).item() == slop_phrase_starting_token:\n",
    "                    if self.slow_debug:\n",
    "                        debug_info = f\"[INFO] Slop phrase '{self.tokenizer.decode(matched_sequence)}' prob was downregulated {round(1/adjustment, 2)}x but still selected.\"\n",
    "                        self._display_debug(debug_info)\n",
    "                        time.sleep(self.debug_delay)\n",
    "                    continue\n",
    "\n",
    "                # Backtrack: remove tokens from the generated_sequence that are part of the disallowed sequence\n",
    "                for _ in range(len(matched_sequence)):\n",
    "                    generated_sequence.pop()\n",
    "                    current_position -= 1\n",
    "                    num_new_tokens -= 1\n",
    "\n",
    "                # Clear the logit_cache ahead of start_pos since we've backtracked\n",
    "                to_del = [key for key in self.logit_cache if key > start_pos]\n",
    "                for key in to_del:\n",
    "                    del self.logit_cache[key]\n",
    "\n",
    "                continue  # Continue to the next iteration\n",
    "\n",
    "        # Final display of the generated text\n",
    "        final_text = self.tokenizer.decode(generated_sequence[prompt_length:])\n",
    "        if self.inference_output:\n",
    "            with self.inference_output:\n",
    "                self.inference_output.clear_output(wait=True)\n",
    "                display(HTML(f\"<div style='white-space: pre-wrap;'>{final_text}</div>\"))\n",
    "        yield generated_sequence\n",
    "\n",
    "        # Clear variables to free up memory\n",
    "        del next_token_logits, filtered_logits\n",
    "\n",
    "    def _filter_logits(self, logits: torch.FloatTensor, top_k: int, top_p: float, min_p: float) -> torch.FloatTensor:\n",
    "        # Apply min_p filtering\n",
    "        if min_p is not None:\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            top_prob, _ = torch.max(probs, dim=-1)\n",
    "            scaled_min_p = min_p * top_prob\n",
    "            logits = torch.where(probs < scaled_min_p, float('-inf'), logits)\n",
    "\n",
    "        if top_k is not None and top_k > 0:\n",
    "            top_k = min(top_k, logits.size(-1))\n",
    "            top_k_logits, _ = torch.topk(logits, top_k)\n",
    "            min_top_k = top_k_logits[:, -1].unsqueeze(-1)\n",
    "            logits = torch.where(logits < min_top_k, float('-inf'), logits)\n",
    "\n",
    "        if top_p is not None and top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "            sorted_indices_to_remove[:, 0] = False\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                dim=1, index=sorted_indices, src=sorted_indices_to_remove\n",
    "            )\n",
    "            logits = logits.masked_fill(indices_to_remove, float('-inf'))\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def _display_debug(self, message: str):\n",
    "        \"\"\"\n",
    "        Displays debug information in the debug_output widget.\n",
    "        \"\"\"\n",
    "        if self.debug_output:\n",
    "            with self.debug_output:\n",
    "                self.debug_output.clear_output(wait=True)\n",
    "                display(HTML(f\"<pre>{message}</pre>\"))\n",
    "        else:\n",
    "            print(message)\n",
    "\n",
    "def chat_antislop(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    messages: List[Dict[str, str]],\n",
    "    max_length: int = None,\n",
    "    max_new_tokens: int = None,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = None,\n",
    "    top_p: float = None,\n",
    "    min_p: float = None,\n",
    "    slop_phrase_prob_adjustments: Dict[str, float] = None,\n",
    "    adjustment_strength: float = 1.0,  # 1.0 is no change from the provided adjustment factors.\n",
    "    device: torch.device = torch.device('cuda'),\n",
    "    streaming: bool = False,\n",
    "    slow_debug: bool = False,  # Add slow_debug argument for debugging\n",
    "    output_every_n_tokens: int = 1,  # Control how frequently the output is updated\n",
    "    debug_delay: float = 2.0,  # Delay for slow debugging mode\n",
    "    inference_output: Output = None,  # For visualization during generation\n",
    "    debug_output: Output = None,  # For visualization of debug information\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a chat response while avoiding overrepresented phrases (slop) with debugging features.\n",
    "    This method creates a generator or a non-streamed output, depending on the streaming flag.\n",
    "\n",
    "    Args:\n",
    "        model (PreTrainedModel): The language model.\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer.\n",
    "        messages (List[Dict[str, str]]): The list of messages in the conversation.\n",
    "        max_length (int, optional): The maximum length of the generated text (including the prompt).\n",
    "        max_new_tokens (int, optional): The maximum number of new tokens to generate.\n",
    "        temperature (float): Sampling temperature.\n",
    "        top_k (int): Top-k filtering.\n",
    "        top_p (float): Top-p (nucleus) filtering.\n",
    "        min_p (float): Minimum probability filtering.\n",
    "        slop_phrase_prob_adjustments (Dict[str, float], optional): Dictionary of target words with their respective probability adjustment factor.\n",
    "        adjustment_strength (float, optional): Strength of the downregulation adjustment.\n",
    "        device (torch.device, optional): The device to run the model on.\n",
    "        streaming (bool, optional): Whether to yield tokens as they are generated.\n",
    "        slow_debug (bool, optional): Enables slow debug mode when set to True.\n",
    "        output_every_n_tokens (int, optional): Frequency of updating the inference output display.\n",
    "        debug_delay (float, optional): Time in seconds to pause during slow debug steps.\n",
    "        inference_output (Output, optional): For visualization during generation.\n",
    "        debug_output (Output, optional): For visualization of debug information.\n",
    "\n",
    "    Returns:\n",
    "        Union[Generator[str, None, None], List[int]]:\n",
    "            If streaming is True, yields generated text chunks.\n",
    "            If streaming is False, returns a list of generated token IDs.\n",
    "    \"\"\"\n",
    "    # Build the prompt using the provided messages\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Initialize the custom word sampler\n",
    "    sampler = AdvancedCustomWordSampler(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        slop_phrase_prob_adjustments=slop_phrase_prob_adjustments or {},\n",
    "        starting_tokens_lookup=precompute_starting_tokens(tokenizer, slop_phrase_prob_adjustments or {}),\n",
    "        adjustment_strength=adjustment_strength,\n",
    "        device=device,\n",
    "        slow_debug=slow_debug,  # Pass slow_debug for detailed debug output\n",
    "        output_every_n_tokens=output_every_n_tokens,\n",
    "        debug_delay=debug_delay,\n",
    "        inference_output=inference_output,\n",
    "        debug_output=debug_output,\n",
    "    )\n",
    "\n",
    "    if streaming:\n",
    "        return sampler.generate_stream(\n",
    "            prompt=prompt,\n",
    "            max_length=max_length,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            min_p=min_p,\n",
    "        )\n",
    "    else:\n",
    "        # Collect all tokens and return as a final result\n",
    "        generated_tokens = []\n",
    "        for token_sequence in sampler.generate_stream(\n",
    "            prompt=prompt,\n",
    "            max_length=max_length,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            min_p=min_p,\n",
    "        ):\n",
    "            generated_tokens.append(token_sequence)\n",
    "        return generated_tokens\n",
    "    \n",
    "\n",
    "def generate_antislop(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    prompt: str,\n",
    "    max_length: int = None,\n",
    "    max_new_tokens: int = None,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = None,\n",
    "    top_p: float = None,\n",
    "    min_p: float = None,\n",
    "    slop_phrase_prob_adjustments: Dict[str, float] = None,\n",
    "    adjustment_strength: float = 1.0,\n",
    "    device: torch.device = torch.device('cuda'),\n",
    "    streaming: bool = False,\n",
    "    slow_debug: bool = False,  # Added slow_debug\n",
    "    output_every_n_tokens: int = 1,\n",
    "    debug_delay: float = 2.0,\n",
    "    inference_output: Output = None,\n",
    "    debug_output: Output = None,\n",
    ") -> Union[Generator[str, None, None], List[int]]:\n",
    "    \"\"\"\n",
    "    Wrapper function for generate_antislop that handles both streaming and non-streaming modes.\n",
    "    \"\"\"\n",
    "    # Type checking and validation of input arguments\n",
    "    if not isinstance(prompt, str):\n",
    "        raise TypeError(\"prompt must be a string\")\n",
    "    if max_length is not None and not isinstance(max_length, int):\n",
    "        raise TypeError(\"max_length must be an integer or None\")\n",
    "    if max_new_tokens is not None and not isinstance(max_new_tokens, int):\n",
    "        raise TypeError(\"max_new_tokens must be an integer or None\")\n",
    "    if not isinstance(temperature, (int, float)):\n",
    "        raise TypeError(\"temperature must be a float\")\n",
    "    if top_k is not None and not isinstance(top_k, int):\n",
    "        raise TypeError(\"top_k must be an integer or None\")\n",
    "    if top_p is not None and not isinstance(top_p, float):\n",
    "        raise TypeError(\"top_p must be a float or None\")\n",
    "    if min_p is not None and not isinstance(min_p, float):\n",
    "        raise TypeError(\"min_p must be a float or None\")\n",
    "    if slop_phrase_prob_adjustments is not None and not isinstance(slop_phrase_prob_adjustments, dict):\n",
    "        raise TypeError(\"slop_phrase_prob_adjustments must be a dictionary or None\")\n",
    "    if not isinstance(adjustment_strength, (int, float)):\n",
    "        raise TypeError(\"adjustment_strength must be a float\")\n",
    "    if not isinstance(device, torch.device):\n",
    "        raise TypeError(\"device must be an instance of torch.device\")\n",
    "    if not isinstance(streaming, bool):\n",
    "        raise TypeError(\"streaming must be a boolean\")\n",
    "\n",
    "    # Value validation\n",
    "    if max_length is not None and max_length <= 0:\n",
    "        raise ValueError(\"max_length must be positive\")\n",
    "    if max_new_tokens is not None and max_new_tokens <= 0:\n",
    "        raise ValueError(\"max_new_tokens must be positive\")\n",
    "    if temperature <= 0:\n",
    "        raise ValueError(\"temperature must be > 0\")\n",
    "    if top_k is not None and top_k <= 0:\n",
    "        raise ValueError(\"top_k must be positive\")\n",
    "    if top_p is not None and (top_p <= 0 or top_p > 1):\n",
    "        raise ValueError(\"top_p must be in the range (0, 1]\")\n",
    "    if min_p is not None and (min_p <= 0 or min_p > 1):\n",
    "        raise ValueError(\"min_p must be in the range (0, 1]\")\n",
    "    if adjustment_strength < 0:\n",
    "        raise ValueError(\"adjustment_strength must be non-negative\")\n",
    "\n",
    "    if slop_phrase_prob_adjustments:\n",
    "        for phrase, adjustment in slop_phrase_prob_adjustments.items():\n",
    "            if not isinstance(phrase, str):\n",
    "                raise TypeError(\"All keys in slop_phrase_prob_adjustments must be strings\")\n",
    "            if not isinstance(adjustment, (int, float)):\n",
    "                raise TypeError(\"All values in slop_phrase_prob_adjustments must be floats\")\n",
    "\n",
    "    if streaming:\n",
    "        return _generate_antislop(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_length=max_length,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            min_p=min_p,\n",
    "            slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "            adjustment_strength=adjustment_strength,\n",
    "            device=device,\n",
    "            slow_debug=slow_debug,  # Pass slow_debug to support detailed debug output\n",
    "            output_every_n_tokens=output_every_n_tokens,\n",
    "            debug_delay=debug_delay,\n",
    "            inference_output=inference_output,\n",
    "            debug_output=debug_output,\n",
    "            streaming=True\n",
    "        )\n",
    "    else:\n",
    "        generated_tokens = []\n",
    "        for token in _generate_antislop(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_length=max_length,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            min_p=min_p,\n",
    "            slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "            adjustment_strength=adjustment_strength,\n",
    "            device=device,\n",
    "            slow_debug=slow_debug,  # Pass slow_debug to support detailed debug output\n",
    "            output_every_n_tokens=output_every_n_tokens,\n",
    "            debug_delay=debug_delay,\n",
    "            inference_output=inference_output,\n",
    "            debug_output=debug_output,\n",
    "            streaming=True  # Always stream internally\n",
    "        ):\n",
    "            generated_tokens.append(token)\n",
    "        return generated_tokens\n",
    "\n",
    "\n",
    "def _generate_antislop(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    prompt: str,\n",
    "    max_length: int = None,\n",
    "    max_new_tokens: int = None,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = None,\n",
    "    top_p: float = None,\n",
    "    min_p: float = None,\n",
    "    slop_phrase_prob_adjustments: Dict[str, float] = None,\n",
    "    adjustment_strength: float = 1.0,\n",
    "    device: torch.device = torch.device('cuda'),\n",
    "    slow_debug: bool = False,  # Added slow_debug\n",
    "    output_every_n_tokens: int = 1,\n",
    "    debug_delay: float = 2.0,\n",
    "    inference_output: Output = None,\n",
    "    debug_output: Output = None,\n",
    "    streaming: bool = False\n",
    ") -> Generator[int, None, None]:\n",
    "    \"\"\"\n",
    "    Generates text while avoiding overrepresented phrases (slop).\n",
    "    This function is now always a generator.\n",
    "    \"\"\"\n",
    "    # Precompute starting tokens for the slop phrases\n",
    "    starting_tokens_lookup = precompute_starting_tokens(tokenizer, slop_phrase_prob_adjustments or {})\n",
    "\n",
    "    # Initialize the sampler\n",
    "    sampler = AdvancedCustomWordSampler(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        slop_phrase_prob_adjustments=slop_phrase_prob_adjustments or {},\n",
    "        starting_tokens_lookup=starting_tokens_lookup,\n",
    "        adjustment_strength=adjustment_strength,\n",
    "        device=device,\n",
    "        slow_debug=slow_debug,  # Enable slow debugging\n",
    "        output_every_n_tokens=output_every_n_tokens,\n",
    "        debug_delay=debug_delay,\n",
    "        inference_output=inference_output,\n",
    "        debug_output=debug_output,\n",
    "    )\n",
    "\n",
    "    # Generate token stream\n",
    "    token_stream = sampler.generate_stream(\n",
    "        prompt=prompt,\n",
    "        max_length=max_length,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        min_p=min_p\n",
    "    )\n",
    "\n",
    "    num_new_tokens = 0\n",
    "    for generated_sequence in token_stream:\n",
    "        current_length = len(generated_sequence)\n",
    "        # Stream out tokens\n",
    "        for token in generated_sequence:\n",
    "            yield token\n",
    "            num_new_tokens += 1\n",
    "\n",
    "            # Stop generation if we hit max_new_tokens\n",
    "            if max_new_tokens is not None and num_new_tokens >= max_new_tokens:\n",
    "                return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Prompt</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62a4dc19dc24ec6b0c5b6398699e00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Inference Output</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159690c6397f48879ceab35646798b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Debug Information</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bfcd2b218c4461dbf38268f1c88b875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example prompt for the model\n",
    "prompt = \"Once upon a time, in a bustling city of Technopolis, there lived a weaver named Elara.\"\n",
    "\n",
    "# Define the messages for a chat scenario (for example purposes)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a story about Elara, the weaver of tapestries.\"}\n",
    "]\n",
    "\n",
    "\n",
    "# Display the output widgets\n",
    "prompt_output = Output()\n",
    "inference_output = Output()\n",
    "debug_output = Output()\n",
    "\n",
    "display(HTML(\"<h2>Prompt</h2>\"))\n",
    "display(prompt_output)\n",
    "display(HTML(\"<h2>Inference Output</h2>\"))\n",
    "display(inference_output)\n",
    "display(HTML(\"<h2>Debug Information</h2>\"))\n",
    "display(debug_output)\n",
    "\n",
    "# Initialize display (if using in Jupyter)\n",
    "with prompt_output:\n",
    "    prompt_output.clear_output(wait=True)\n",
    "    display(HTML(f\"<div style='white-space: pre-wrap;'>{prompt}</div>\"))\n",
    "\n",
    "# Enable slow debug mode\n",
    "SLOW_DEBUG = True\n",
    "\n",
    "# Call the chat_antislop to generate the story with the given messages\n",
    "try:\n",
    "    for chunk in chat_antislop(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        messages=messages,\n",
    "        max_new_tokens=200,        \n",
    "        temperature=1,\n",
    "        min_p=0.1,\n",
    "        slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "        adjustment_strength=5.0,\n",
    "        device=device,\n",
    "        streaming=True,\n",
    "        slow_debug=True,  # Enable slow debugging\n",
    "        output_every_n_tokens=5,  # Update every 5 tokens\n",
    "        debug_delay=1.0,  # Set delay for debug output\n",
    "        inference_output=inference_output,  # Visualization of the text output\n",
    "        debug_output=debug_output  # Visualization of the debug information\n",
    "    ):\n",
    "        pass  # Text is already being visualized in the widgets\n",
    "except Exception as e:\n",
    "    with debug_output:\n",
    "        debug_output.clear_output(wait=True)\n",
    "        debug_output.append_stdout(f\"\\n\\nAn error occurred: {str(e)}\\n\")\n",
    "        traceback.print_exc(file=sys.stdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
