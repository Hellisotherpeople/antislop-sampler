{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ! Important\n",
    "This notebook contains an unfinished version of the sampler that contains bugs and vram leaks. You should use the code in antislop_generate.py (see example_generate.ipynb also) if you want to actually use the sampler for anything.\n",
    "\n",
    "I'm leaving it here for now because it's a cool way to visualise the backtracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import traceback\n",
    "from typing import List, Dict, Tuple, Generator, Set\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from ipywidgets import Output\n",
    "import numpy as np\n",
    "\n",
    "# Enable efficient transfer for Hugging Face models\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = \"1\"\n",
    "\n",
    "# Set the device to 'cuda' if available, else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify the model name (replace with your preferred model)\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are automatically derived from relative frequency of words in a gpt generated dataset. See slopcalc.ipynb for the code to generate a more complete list.\n",
    "slop_phrase_prob_adjustments = [['kaleidoscope', 0.5], ['symphony', 0.5], ['testament to', 0.5], ['moth to a flame', 0.5], ['canvas', 0.5], ['eyes glinted', 0.5], ['camaraderie', 0.5], ['humble abode', 0.5], ['cold and calculating', 0.5], ['eyes never leaving', 0.5], ['tapestry', 0.5], ['barely above a whisper', 0.5], ['body and soul', 0.5], ['orchestra', 0.5], ['depths', 0.5], ['a dance of', 0.5], ['chuckles darkly', 0.5], ['maybe, just maybe', 0.5], ['maybe that was enough', 0.5], ['with a mixture of', 0.5], ['air was filled with anticipation', 0.5], ['cacophony', 0.5], ['bore silent witness to', 0.5], ['eyes sparkling with mischief', 0.5], ['was only just beginning', 0.5], ['practiced ease', 0.5], ['ready for the challenges', 0.5], ['only just getting started', 0.5], ['once upon a time', 0.5], ['nestled deep within', 0.5], ['ethereal beauty', 0.5], ['life would never be the same again.', 0.5], [\"it's important to remember\", 0.5], ['for what seemed like an eternity', 0.5], ['feel a sense of pride and accomplishment', 0.5], ['little did he know', 0.5], ['ball is in your court', 0.5], ['game is on', 0.5], ['choice is yours', 0.5], ['feels like an electric shock', 0.5], ['threatens to consume', 0.5], ['meticulous', 0.5], ['meticulously', 0.5], ['navigating', 0.5], ['complexities', 0.5], ['realm', 0.5], ['understanding', 0.5], ['dive into', 0.5], ['shall', 0.5], ['tailored', 0.5]];\n",
    "slop_phrase_prob_adjustments = dict(slop_phrase_prob_adjustments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('slop_phrase_prob_adjustments.json'):\n",
    "    with open('slop_phrase_prob_adjustments.json', 'r') as f:\n",
    "        slop_phrase_prob_adjustments = dict(json.load(f)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precompute_starting_tokens(\n",
    "    tokenizer: PreTrainedTokenizer, slop_phrase_prob_adjustments: Dict[str, float]\n",
    ") -> Dict[Tuple[int, ...], Set[int]]:\n",
    "    \"\"\"\n",
    "    Precompute all starting token IDs for each target word variant.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer used by the model.\n",
    "        word_penalties (Dict[str, float]): Dictionary of target words with their respective penalty.\n",
    "\n",
    "    Returns:\n",
    "        Dict[Tuple[int, ...], Set[int]]: A mapping from each token sequence (word variant) to a set of starting token IDs.\n",
    "    \"\"\"\n",
    "    starting_tokens_lookup = {}\n",
    "\n",
    "    for word in slop_phrase_prob_adjustments.keys():\n",
    "        variants = [\n",
    "            word.lower(),\n",
    "            word.capitalize(),\n",
    "            word.upper(),\n",
    "            f\" {word.lower()}\",\n",
    "            f\" {word.capitalize()}\",\n",
    "            f\" {word.upper()}\",\n",
    "        ]\n",
    "\n",
    "        for variant in variants:\n",
    "            # Encode the full variant\n",
    "            token_ids = tokenizer.encode(variant, add_special_tokens=False)\n",
    "            starting_tokens = set()\n",
    "            if token_ids:\n",
    "                starting_tokens.add(token_ids[0])\n",
    "                first_token_decoded = tokenizer.decode(token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                # Iterate over all possible prefixes of the first token\n",
    "                for i in range(len(first_token_decoded) - 1):\n",
    "                    prefix = first_token_decoded[:-(i + 1)]\n",
    "                    encoded_prefix = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "                    if encoded_prefix:\n",
    "                        starting_tokens.add(encoded_prefix[0])  # Add the first token of the prefix\n",
    "\n",
    "                starting_tokens_lookup[tuple(token_ids)] = starting_tokens\n",
    "\n",
    "    return starting_tokens_lookup\n",
    "\n",
    "# Precompute starting tokens\n",
    "starting_tokens_lookup = precompute_starting_tokens(tokenizer, slop_phrase_prob_adjustments)\n",
    "\n",
    "class AdvancedCustomWordSampler:\n",
    "    \"\"\"\n",
    "    A sampler that generates text while downregulating specified words or phrases.\n",
    "    It uses backtracking and custom adjustments to avoid overrepresented words.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        slop_phrase_prob_adjustments: Dict[str, float],\n",
    "        starting_tokens_lookup: Dict[Tuple[int, ...], Set[int]],\n",
    "        adjustment_strength: float = 1.0,\n",
    "        device: torch.device = torch.device('cuda'),\n",
    "        output_every_n_tokens: int = 5,\n",
    "        slow_debug: bool = False,\n",
    "        debug_delay: float = 2.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the AdvancedCustomWordSampler with the necessary parameters.\n",
    "\n",
    "        Args:\n",
    "            model (PreTrainedModel): The language model to use for Generation.\n",
    "            tokenizer (PreTrainedTokenizer): The tokenizer associated with the model.\n",
    "            slop_phrase_prob_adjustments (Dict[str, float]): Dictionary of target words with their respective probability adjustment factor.\n",
    "            starting_tokens_lookup (Dict[Tuple[int, ...], Set[int]]): Mapping from token sequences to starting token IDs.\n",
    "            adjustment_strength (float): Strength of the downregulation adjustment.\n",
    "            device (torch.device): Device to run the model on.\n",
    "            output_every_n_tokens (int): Frequency of updating the inference output display.\n",
    "            slow_debug (bool): Enables slow debug mode when set to True.\n",
    "            debug_delay (float): Time in seconds to pause during slow debug steps.\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.slop_phrase_prob_adjustments = slop_phrase_prob_adjustments\n",
    "        self.starting_tokens_lookup = starting_tokens_lookup\n",
    "        self.adjustment_strength = adjustment_strength\n",
    "        self.device = device\n",
    "        self.output_every_n_tokens = output_every_n_tokens\n",
    "        self.slow_debug = slow_debug\n",
    "        self.debug_delay = debug_delay\n",
    "\n",
    "        # Prepare token sequences for downregulation\n",
    "        self.token_sequences = self._prepare_token_sequences()\n",
    "        self.max_sequence_length = max(len(seq) for seq in self.token_sequences.keys())\n",
    "\n",
    "        # Initialize a cache to store logits along with their positions\n",
    "        self.logit_cache = {}\n",
    "\n",
    "        # Record of downregulated sequences at specific positions\n",
    "        self.downregulated_positions = {}  # Key: position, Value: set of sequences\n",
    "\n",
    "    def _prepare_token_sequences(self) -> Dict[Tuple[int, ...], float]:\n",
    "        \"\"\"\n",
    "        Prepares the token sequences from slop_phrase_prob_adjustments for efficient lookup.\n",
    "\n",
    "        Returns:\n",
    "            Dict[Tuple[int, ...], float]: Mapping from token ID sequences to their adjustment factors.\n",
    "        \"\"\"\n",
    "        token_sequences = {}\n",
    "        for word, prob_adjustment_factor in self.slop_phrase_prob_adjustments.items():\n",
    "            variants = [\n",
    "                word.lower(),\n",
    "                word.capitalize(),\n",
    "                word.upper(),\n",
    "                f\" {word.lower()}\",\n",
    "                f\" {word.capitalize()}\",\n",
    "                f\" {word.upper()}\",\n",
    "            ]\n",
    "            for variant in variants:\n",
    "                token_ids = tuple(self.tokenizer.encode(variant, add_special_tokens=False))\n",
    "                if token_ids:\n",
    "                    token_sequences[token_ids] = prob_adjustment_factor\n",
    "        return token_sequences\n",
    "\n",
    "    def _adjust_logits(self, logits: torch.FloatTensor, adjustment: float) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Adjusts the logits by applying the downregulation factor.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.FloatTensor): The original logits.\n",
    "            adjustment (float): The adjustment factor to apply.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: The adjusted logits.\n",
    "        \"\"\"\n",
    "        log_adjustment = torch.log(torch.tensor(adjustment ** self.adjustment_strength, device=self.device))\n",
    "        return logits + log_adjustment  # Lowering the logit for disallowed tokens\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_length: int,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 0.95,\n",
    "    ) -> Generator[str, None, None]:\n",
    "        \"\"\"\n",
    "        Generates text in a streaming fashion with custom downregulation and backtracking.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The initial text prompt.\n",
    "            max_length (int): The maximum length of the generated text.\n",
    "            temperature (float): Sampling temperature.\n",
    "            top_k (int): Top-k filtering.\n",
    "            top_p (float): Top-p (nucleus) filtering.\n",
    "\n",
    "        Yields:\n",
    "            Generator[str, None, None]: Yields generated text chunks.\n",
    "        \"\"\"\n",
    "        # Encode the prompt\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        generated_sequence = input_ids[0].tolist()\n",
    "        current_position = len(generated_sequence)  # Tracks the current position in the sequence\n",
    "\n",
    "        # Initialize display (if using in Jupyter)\n",
    "        with inference_output:\n",
    "            inference_output.clear_output(wait=True)\n",
    "            display(HTML(f\"<div style='white-space: pre-wrap;'>{self.tokenizer.decode(generated_sequence)}</div>\"))\n",
    "\n",
    "        past_key_values = None\n",
    "        output_tokens_counter = 0\n",
    "\n",
    "        while len(generated_sequence) < max_length:\n",
    "            # Ensure the generated token ids are tokenised how the tokeniser would normally do it,\n",
    "            # We do this ecause our lookup only works when the text is tokenised the normal way, \n",
    "            # and the model may have arrived at the slop phrase by a different route.\n",
    "            # !! actually -- this might break things (?), as cached values are indexed by position. need to rethink this.\n",
    "            #generated_sequence = self.tokenizer.encode(self.tokenizer.decode(generated_sequence, skip_special_tokens=False), add_special_tokens=False)\n",
    "\n",
    "            current_input_ids = torch.tensor([generated_sequence], device=self.device)\n",
    "\n",
    "            regenerating = False\n",
    "            if current_position in self.logit_cache:\n",
    "                # We backtracked and want to use the cached logits\n",
    "                next_token_logits = self.logit_cache[current_position]\n",
    "                past_key_values = None\n",
    "                regenerating = True\n",
    "            else:\n",
    "                if past_key_values is None:\n",
    "                    outputs = self.model(current_input_ids, use_cache=True)\n",
    "                else:\n",
    "                    outputs = self.model(current_input_ids[:, -1:], past_key_values=past_key_values, use_cache=True)\n",
    "\n",
    "                next_token_logits = outputs.logits[:, -1, :] / temperature\n",
    "                past_key_values = outputs.past_key_values\n",
    "                self.logit_cache[current_position] = next_token_logits.clone()\n",
    "\n",
    "            # Apply top-k and top-p filtering\n",
    "            filtered_logits = self._filter_logits(next_token_logits, top_k, top_p)\n",
    "\n",
    "            # Sample the next token\n",
    "            probs = torch.softmax(filtered_logits, dim=-1)\n",
    "            next_token_index = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = next_token_index.item()\n",
    "\n",
    "            if regenerating:\n",
    "                with debug_output:\n",
    "                    debug_output.clear_output(wait=True)\n",
    "                alt_token = tokenizer.decode(next_token, skip_special_tokens=True)\n",
    "                debug_info = f\"Alternate token: {alt_token}\"\n",
    "                self._display_debug(debug_info)\n",
    "                if self.slow_debug:\n",
    "                    time.sleep(self.debug_delay)\n",
    "\n",
    "            # Append the new token to the sequence\n",
    "            generated_sequence.append(next_token)\n",
    "            current_position += 1\n",
    "            output_tokens_counter += 1\n",
    "\n",
    "            # Yield the current text chunk\n",
    "            current_text = self.tokenizer.decode(generated_sequence)\n",
    "            if output_tokens_counter >= self.output_every_n_tokens:\n",
    "                output_tokens_counter = 0\n",
    "                with inference_output:\n",
    "                    inference_output.clear_output(wait=True)\n",
    "                    display(HTML(f\"<div style='white-space: pre-wrap;'>{current_text}</div>\"))\n",
    "                yield current_text  # Yield the generated text chunk\n",
    "\n",
    "            # Check for end-of-sequence token\n",
    "            if next_token == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # After adding the token, check for disallowed sequences\n",
    "            matched_sequence, start_pos = self._detect_disallowed_sequence(generated_sequence)\n",
    "\n",
    "            if matched_sequence:\n",
    "                # Downregulate the relevant tokens at the start_pos\n",
    "                adjustment = self.token_sequences[matched_sequence]\n",
    "                word = self.tokenizer.decode(torch.tensor(matched_sequence))\n",
    "\n",
    "                # Display debug information\n",
    "                debug_info = f\"Replacing '{word}'\"\n",
    "                self._display_debug(debug_info)\n",
    "                with inference_output:\n",
    "                    inference_output.clear_output(wait=True)\n",
    "                    display(HTML(f\"<div style='white-space: pre-wrap;'>{current_text}</div>\"))\n",
    "\n",
    "                if self.slow_debug:\n",
    "                    time.sleep(self.debug_delay)\n",
    "                    with debug_output:\n",
    "                        debug_output.clear_output(wait=True)\n",
    "\n",
    "                # Identify starting tokens to downregulate\n",
    "                starting_tokens = self.starting_tokens_lookup.get(matched_sequence, set())\n",
    "\n",
    "                for token_id in starting_tokens:\n",
    "                    self.logit_cache[start_pos][:, token_id] *= adjustment\n",
    "\n",
    "                # Record that this sequence has been downregulated at this position\n",
    "                if start_pos not in self.downregulated_positions:\n",
    "                    self.downregulated_positions[start_pos] = set()\n",
    "                self.downregulated_positions[start_pos].add(matched_sequence)\n",
    "\n",
    "                # Check if the starting token would still be selected after downregulation\n",
    "                slop_phrase_starting_token = generated_sequence[start_pos]\n",
    "                if torch.argmax(self.logit_cache[start_pos]).item() == slop_phrase_starting_token:\n",
    "                    if self.slow_debug:\n",
    "                        debug_info = f\"[INFO] Slop phrase '{self.tokenizer.decode(matched_sequence)}' prob was downregulated {round(1/adjustment, 2)}x but still selected.\"\n",
    "                        self._display_debug(debug_info)\n",
    "                        time.sleep(self.debug_delay)\n",
    "                    continue\n",
    "\n",
    "                # Backtrack: remove tokens from the generated_sequence that are part of the disallowed sequence\n",
    "                for _ in range(len(matched_sequence)):\n",
    "                    generated_sequence.pop()\n",
    "                    current_position -= 1\n",
    "\n",
    "                # Update the model's past_key_values by re-encoding up to start_pos\n",
    "                # This is necessary because we've modified the generated_sequence\n",
    "                new_input_ids = torch.tensor([generated_sequence], device=self.device)\n",
    "                outputs = self.model(new_input_ids, use_cache=True)\n",
    "                past_key_values = outputs.past_key_values\n",
    "\n",
    "                # Clear the logit_cache ahead of start_pos since we've backtracked\n",
    "                to_del = [key for key in self.logit_cache if key > start_pos]\n",
    "                for key in to_del:\n",
    "                    del self.logit_cache[key]\n",
    "\n",
    "                continue  # Continue to the next iteration\n",
    "\n",
    "        # Final display of the generated text\n",
    "        final_text = self.tokenizer.decode(generated_sequence)\n",
    "        with inference_output:\n",
    "            inference_output.clear_output(wait=True)\n",
    "            display(HTML(f\"<div style='white-space: pre-wrap;'>{final_text}</div>\"))\n",
    "        yield final_text\n",
    "\n",
    "        # Clear variables to free up memory\n",
    "        del outputs, next_token_logits, filtered_logits, past_key_values\n",
    "\n",
    "    def _filter_logits(self, logits: torch.FloatTensor, top_k: int, top_p: float) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Applies top-k and top-p (nucleus) filtering to the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.FloatTensor): The original logits.\n",
    "            top_k (int): The number of top tokens to keep.\n",
    "            top_p (float): The cumulative probability threshold.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: The filtered logits.\n",
    "        \"\"\"\n",
    "        # Apply top-k\n",
    "        if top_k > 0:\n",
    "            top_k = min(top_k, logits.size(-1))\n",
    "            top_k_logits, _ = torch.topk(logits, top_k)\n",
    "            min_top_k = top_k_logits[:, -1].unsqueeze(-1)\n",
    "            logits = torch.where(logits < min_top_k, float('-inf'), logits)\n",
    "\n",
    "        # Apply top-p\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "            # Remove tokens with cumulative probability above the threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            # Shift the mask right to keep the first token above the threshold\n",
    "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "            sorted_indices_to_remove[:, 0] = False\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                dim=1, index=sorted_indices, src=sorted_indices_to_remove\n",
    "            )\n",
    "            logits = logits.masked_fill(indices_to_remove, float('-inf'))\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def _detect_disallowed_sequence(self, generated_sequence: List[int]) -> Tuple[Tuple[int, ...], int]:\n",
    "        \"\"\"\n",
    "        Detects if the recent tokens in the generated_sequence match any disallowed sequence.\n",
    "\n",
    "        Args:\n",
    "            generated_sequence (List[int]): The list of generated token IDs.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[int, ...], int]: The matched disallowed sequence and its start position.\n",
    "                                         Returns (None, -1) if no match is found.\n",
    "        \"\"\"        \n",
    "        # Start checking from the longest possible sequence to the shortest\n",
    "        for seq_length in range(self.max_sequence_length, 0, -1):\n",
    "            if len(generated_sequence) < seq_length:\n",
    "                continue\n",
    "            candidate_sequence = tuple(generated_sequence[-seq_length:])\n",
    "            if candidate_sequence in self.token_sequences:\n",
    "                start_pos = len(generated_sequence) - seq_length\n",
    "                return candidate_sequence, start_pos\n",
    "        return None, -1\n",
    "\n",
    "    def _display_debug(self, message: str):\n",
    "        \"\"\"\n",
    "        Displays debug information in the debug_output widget.\n",
    "\n",
    "        Args:\n",
    "            message (str): The debug message to display.\n",
    "        \"\"\"\n",
    "        with debug_output:\n",
    "            debug_output.clear_output(wait=True)\n",
    "            display(HTML(f\"<pre>{message}</pre>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Prompt</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6244dac146af4c3d9ae17105d14e914a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Inference Output</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6af435fb6749e18dda3203809403aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Debug Information</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed79539ad9d64f2abd697dd4e4804622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup separate output widgets for prompt, inference, and debug\n",
    "prompt_output = Output()\n",
    "inference_output = Output()\n",
    "debug_output = Output()\n",
    "\n",
    "# Display the output widgets\n",
    "display(HTML(\"<h2>Prompt</h2>\"))\n",
    "display(prompt_output)\n",
    "display(HTML(\"<h2>Inference Output</h2>\"))\n",
    "display(inference_output)\n",
    "display(HTML(\"<h2>Debug Information</h2>\"))\n",
    "display(debug_output)\n",
    "\n",
    "# Enable slow debug mode\n",
    "SLOW_DEBUG = True\n",
    "\n",
    "# Initialize the sampler\n",
    "sampler = AdvancedCustomWordSampler(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "    starting_tokens_lookup=starting_tokens_lookup,\n",
    "    adjustment_strength=1.0,\n",
    "    device=device,\n",
    "    output_every_n_tokens=5,\n",
    "    slow_debug=SLOW_DEBUG,          # Enable slow debug\n",
    "    debug_delay=1.5                # Set delay to 1.5 seconds per debug step\n",
    ")\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Write a story about Elara, the weaver of tapestries in future Technopolis. In the bustling city, a group of \"\n",
    "prompt = \"Once upon a time in a land far away, there was a\"\n",
    "\n",
    "# Display the prompt\n",
    "with prompt_output:\n",
    "    prompt_output.clear_output(wait=True)\n",
    "    display(HTML(f\"<div style='white-space: pre-wrap;'>{prompt}</div>\"))\n",
    "\n",
    "# Start generating\n",
    "try:\n",
    "    for text_chunk in sampler.generate_stream(prompt, max_length=800, temperature=1.0, top_k=50, top_p=0.95):\n",
    "        pass  # The text_chunk is already being displayed via the inference_output widget\n",
    "    print(\"\\nGeneration complete.\")\n",
    "except Exception as e:\n",
    "    with debug_output:\n",
    "        debug_output.clear_output(wait=True)\n",
    "        debug_output.append_stdout(f\"\\n\\nAn error occurred: {str(e)}\\n\")\n",
    "        traceback.print_exc(file=sys.stdout)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
